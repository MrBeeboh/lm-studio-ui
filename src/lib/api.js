/**
 * LM Studio API client.
 * OpenAI-compat: http://localhost:1234/v1
 * REST (load):    http://localhost:1234/api/v1
 *
 * Compatible with LM Studio 0.4.x: parallel inference, /v1/chat/completions, /api/v1/models/load.
 * See docs/LM-STUDIO-0.4-RELEASE-NOTES.md for 0.4.0â€“0.4.2 changelog and impact.
 */

// In dev, use proxy (same origin) to avoid CORS; in build use direct URL
const getBase = () => (typeof import.meta !== 'undefined' && import.meta.env?.DEV ? '/api/lmstudio' : 'http://localhost:1234');
const LM_STUDIO_BASE = `${getBase()}/v1`;
const LM_STUDIO_REST = `${getBase()}/api/v1`;

/**
 * Fetch list of models from LM Studio.
 * Uses LM Studio 0.4.x native GET /api/v1/models (all downloaded LLMs) when available,
 * so Dashboard and main selector show every model, not only currently loaded ones.
 * Falls back to OpenAI-compat GET /v1/models (loaded only) for older LM Studio.
 * @returns {Promise<{ id: string }[]>}
 */
export async function getModels() {
  const res = await fetch(`${LM_STUDIO_REST}/models`);
  if (res.ok) {
    const data = await res.json();
    const models = data.models ?? [];
    const llms = Array.isArray(models)
      ? models.filter((m) => m.type === 'llm').map((m) => ({ id: m.key ?? m.id }))
      : [];
    if (llms.length > 0) return llms;
  }
  const fallback = await fetch(`${LM_STUDIO_BASE}/models`);
  if (!fallback.ok) throw new Error(`LM Studio models: ${fallback.status}`);
  const data = await fallback.json();
  const list = data.data ?? data;
  return Array.isArray(list) ? list.map((m) => ({ id: m.id ?? m.id })) : [];
}

/**
 * Load a model with the given load config (LM Studio REST API).
 * Applies context_length, eval_batch_size, flash_attention, offload_kv_cache_to_gpu.
 * Model will be unloaded and reloaded with the new config.
 * @param {string} modelId - Model identifier (as in GET /v1/models)
 * @param {Object} loadConfig
 * @param {number} [loadConfig.context_length]
 * @param {number} [loadConfig.eval_batch_size]
 * @param {boolean} [loadConfig.flash_attention]
 * @param {boolean} [loadConfig.offload_kv_cache_to_gpu]
 * @param {string|number} [loadConfig.gpu_offload] - 'max' | 'off' | 0-1 (CLI: --gpu)
 * @param {number} [loadConfig.cpu_threads]
 * @returns {Promise<{ type: string, instance_id: string, load_time_seconds: number, status: string, load_config?: object }>}
 */
export async function loadModel(modelId, loadConfig = {}) {
  const body = { model: modelId };
  if (loadConfig.context_length != null) body.context_length = loadConfig.context_length;
  if (loadConfig.eval_batch_size != null) body.eval_batch_size = loadConfig.eval_batch_size;
  if (loadConfig.flash_attention != null) body.flash_attention = loadConfig.flash_attention;
  if (loadConfig.offload_kv_cache_to_gpu != null) body.offload_kv_cache_to_gpu = loadConfig.offload_kv_cache_to_gpu;
  if (loadConfig.gpu_offload != null) {
    const g = loadConfig.gpu_offload;
    body.gpu = g === 'max' || g === 'off' ? g : Number(g);
  }
  if (loadConfig.cpu_threads != null) body.n_threads = loadConfig.cpu_threads;
  body.echo_load_config = true;

  const res = await fetch(`${LM_STUDIO_REST}/models/load`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(body),
  });
  if (!res.ok) {
    const text = await res.text();
    throw new Error(`LM Studio load: ${res.status} ${text}`);
  }
  return res.json();
}

/**
 * Stream a chat completion from LM Studio.
 * @param {Object} opts
 * @param {string} opts.model - Model id
 * @param {Array<{ role: string, content: string|Array }>} opts.messages
 * @param {Object} [opts.options] - temperature, max_tokens, ttl, etc.
 * @param {(chunk: string) => void} opts.onChunk
 * @param {(usage: { prompt_tokens?: number, completion_tokens?: number }) => void} [opts.onUsage]
 * @param {AbortSignal} [opts.signal] - AbortSignal to cancel the stream
 * @returns {Promise<{ usage?: object, elapsedMs: number, aborted?: boolean }>}
 */
export async function streamChatCompletion({ model, messages, options = {}, onChunk, onUsage, signal }) {
  const startTime = Date.now();
  let usage = null;
  try {
    const res = await fetch(`${LM_STUDIO_BASE}/chat/completions`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model,
        messages,
        stream: true,
        temperature: options.temperature ?? 0.7,
        max_tokens: options.max_tokens ?? 4096,
        ...(options.top_p != null && { top_p: options.top_p }),
        ...(options.top_k != null && { top_k: options.top_k }),
        ...(options.repeat_penalty != null && { repeat_penalty: options.repeat_penalty }),
        ...(options.stop?.length && { stop: options.stop }),
        ...(options.ttl != null && Number(options.ttl) > 0 && { ttl: Number(options.ttl) }),
      }),
      signal,
    });
    if (!res.ok) {
      const text = await res.text();
      throw new Error(`LM Studio chat: ${res.status} ${text}`);
    }
    const reader = res.body.getReader();
    const decoder = new TextDecoder();
    let buffer = '';
    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() ?? '';
        for (const line of lines) {
          const trimmed = line.trim();
          if (trimmed.startsWith('data: ')) {
            const payload = trimmed.slice(6);
            if (payload === '[DONE]') continue;
            try {
              const parsed = JSON.parse(payload);
              const choice = parsed.choices?.[0];
              if (choice?.delta?.content) onChunk(choice.delta.content);
              if (parsed.usage) {
                usage = parsed.usage;
                onUsage?.(parsed.usage);
              }
            } catch (_) {}
          }
        }
      }
    } catch (readErr) {
      if (readErr?.name === 'AbortError') {
        return { usage, elapsedMs: Date.now() - startTime, aborted: true };
      }
      throw readErr;
    }
    return { usage, elapsedMs: Date.now() - startTime };
  } catch (err) {
    if (err?.name === 'AbortError') {
      return { usage, elapsedMs: Date.now() - startTime, aborted: true };
    }
    throw err;
  }
}
